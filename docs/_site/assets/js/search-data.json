{"0": {
    "doc": "Chapter 3",
    "title": "Chapter 3",
    "hpath": "Algorithm.DesignManualBook.chap3",
    "content": " ",
    "url": "http://localhost:4000/notes/05774b2e-ebf7-4bbc-8171-ad191ba0ae0a.html",
    "relUrl": "/notes/05774b2e-ebf7-4bbc-8171-ad191ba0ae0a.html"
  },"1": {
    "doc": "Chapter 3",
    "title": "LeetCode",
    "hpath": "Algorithm.DesignManualBook.chap3",
    "content": ". | Count of Smaller Numbers After Self . We have to see how many numbers are smaller to self on the right. Naive: Just interate for each element and check how many elements are smaller on the right for a given index - \\(O(n^2)\\) . Merge Sort : Posting an example for more clarity . void MergeSort(vector&lt;int&gt; &amp;nums, vector&lt;int&gt; &amp;indices, vector&lt;int&gt; &amp;results, int start, int end){ int count = end-start; if(count &gt; 1){ int mid = start + count/2; MergeSort(nums,indices, results, start, mid); MergeSort(nums, indices, results, mid, end); vector&lt;int&gt; tmp; tmp.reserve(count); int idx1 = start, idx2 = mid; int smallcount=0; while(idx1 &lt; mid || idx2 &lt; end){ if((idx2 == end) || (idx1 &lt; mid &amp;&amp; nums[indices[idx1]] &lt;= nums[indices[idx2]])){ tmp.push_back(indices[idx1]); results[indices[idx1]] += smallcount; idx1++; } else{ tmp.push_back(indices[idx2++]); smallcount++; } } move(tmp.begin(), tmp.end(), indices.begin()+start); } } . | Construct Binary Tree from Preorder and Inorder Traversal . | . Two approaches: . | Recursive: Here we keep adding preorder entries into a tree. To differentiate b/w entries which will be in left/right subtree is done on the basis of Inorder vector. Seems \\(O(n^2)\\) as we need to find the inorder entry idx for each preorder idx. | . private: TreeNode* build(vector&lt;int&gt; &amp;preorder, vector&lt;int&gt;&amp; inorder, int start, int end, int&amp; idx){ if(start &gt; end){ return NULL; } TreeNode* node = new TreeNode(preorder[idx++]); int it = find(inorder.begin()+start, inorder.end(), preorder[idx-1]) - inorder.begin(); node-&gt;left = build(preorder, inorder, start, it-1, idx); node-&gt;right = build(preorder, inorder, it+1, end, idx); return node; } public: TreeNode* buildTree(vector&lt;int&gt;&amp; preorder, vector&lt;int&gt;&amp; inorder) { int idx=0; return build(preorder, inorder, 0, inorder.size()-1, idx); } . | Stack : This is new for me. \\(O(n)\\). Here is the initution: . We know that preorder transversal is as follows: . | node | node-&gt;left | node-&gt;right . So we always transverse a left node in preorder until there are none left(I hope you got the pun). How do we know that there is no left subtree after a point? Answer from Inorder transversal . The inorder transversal can be shown as follows: . | node-&gt;left | node | node-&gt;right . so We store the left most node of the tree in the beginning. then the second most and the list goes on. so what we can do is: . | . | Make a stack of nodes with the first entry as preorder[0] | Keep assuming that the next entry is a left child until the top of the stack matches the inorder[0]. this means we have reached the left mode node | now pop the stack until we have s.top() == inorder[j] or s.empty(). | Once this is done, we can see that the left most node of the remaining tree is part of the right subtree of the node. | So we will assign the next node as right child. | to keep track of the right child we can keep a flag which resets after assigning the next node as right child. | . | . public: TreeNode* buildTree(vector&lt;int&gt;&amp; preorder, vector&lt;int&gt;&amp; inorder) { if(!preorder.size()) return NULL; stack&lt;TreeNode*&gt; st; TreeNode* root = new TreeNode(preorder[0]); st.push(root); int i=1, j=0, flag=0; TreeNode* t = root; while(i &lt; preorder.size()){ if(!st.empty() &amp;&amp; st.top()-&gt;val == inorder[j]){ t = st.top(); flag = 1; st.pop(); j++; } else{ if(flag){ flag=0; t-&gt;right = new TreeNode(preorder[i++]); t = t-&gt;right; st.push(t); } else{ t-&gt;left = new TreeNode(preorder[i++]); t = t-&gt;left; st.push(t); } } } return root; } . ",
    "url": "http://localhost:4000/notes/05774b2e-ebf7-4bbc-8171-ad191ba0ae0a.html#leetcode",
    "relUrl": "/notes/05774b2e-ebf7-4bbc-8171-ad191ba0ae0a.html#leetcode"
  },"2": {
    "doc": "Chapter 6",
    "title": "Hashing and Randomized Algorithms",
    "hpath": "Algorithm.DesignManualBook.chap6",
    "content": " ",
    "url": "http://localhost:4000/notes/17562e75-6070-454a-bae1-abfca780bc6d.html#hashing-and-randomized-algorithms",
    "relUrl": "/notes/17562e75-6070-454a-bae1-abfca780bc6d.html#hashing-and-randomized-algorithms"
  },"3": {
    "doc": "Chapter 6",
    "title": "Balls and Bins",
    "hpath": "Algorithm.DesignManualBook.chap6",
    "content": "Hashing can be seen as assigning bucket for a given ball(or value). If we get lucky, we will always have a single ball in each bucket. This ensure we have an expected access time of O(1). Some of the probability related jargon involved has been mentioned in the book. ",
    "url": "http://localhost:4000/notes/17562e75-6070-454a-bae1-abfca780bc6d.html#balls-and-bins",
    "relUrl": "/notes/17562e75-6070-454a-bae1-abfca780bc6d.html#balls-and-bins"
  },"4": {
    "doc": "Chapter 6",
    "title": "Randomization In Hashing",
    "hpath": "Algorithm.DesignManualBook.chap6",
    "content": "In hashing we saw that things can go really wrong for some inputs. To prevent that we add some randomization to the hashing function. This makes sure that worst case in hashing is not attached to a particular input. Rather it is due to really bad luck. So Instead of using simple hashing i.e. \\(h(x) = f(x)mod(m)\\) we add a random number \\(p(f(x) &gt;&gt; p &gt;&gt;m)\\), which provides randomization to our hashing function. \\(h(x) = (f(x)mod(p))mod(m)\\) . ",
    "url": "http://localhost:4000/notes/17562e75-6070-454a-bae1-abfca780bc6d.html#randomization-in-hashing",
    "relUrl": "/notes/17562e75-6070-454a-bae1-abfca780bc6d.html#randomization-in-hashing"
  },"5": {
    "doc": "Chapter 6",
    "title": "Bloom Filters",
    "hpath": "Algorithm.DesignManualBook.chap6",
    "content": "What’s this about? To prevent multiple false positives, we can use this technique to reduce the probability of false positives. In normal hashing, we calculate h(x) and check if the value stored is same as the searched input. Instead of that we can have multiple hashing functions \\(h_1(x), h_2(x), ..., h_k(x)\\). When we search for a value, we calculate all these hashes and check if we get a set bit for each of these hash functions. We can see that for \\(k &gt;= 2\\), we get reduced probabilities . ",
    "url": "http://localhost:4000/notes/17562e75-6070-454a-bae1-abfca780bc6d.html#bloom-filters",
    "relUrl": "/notes/17562e75-6070-454a-bae1-abfca780bc6d.html#bloom-filters"
  },"6": {
    "doc": "Chapter 6",
    "title": "Minwise Hashing",
    "hpath": "Algorithm.DesignManualBook.chap6",
    "content": "This is one of the new things, I learnt from this book. It is mostly using probability to make some conclusions which are useful. Minwise hashing is a clever but simple idea to do this. We compute the hash code \\(h(w_i)\\) of every word in \\(D_1\\), and select the code with smallest value among all the hash codes. Then we do the same thing with the words in \\(D_2\\), using the same hash function. What is cool about this is that it gives a way to pick the same random word in both documents. Suppose the vocabularies of each document were identical. Then the word with minimum hash code will be the same in both \\(D_1\\) and \\(D_2\\), and we get a match. In contrast, suppose we had picked completely random words from each document. Then the probability of picking the same word would be only \\(1/v\\), where \\(v\\) is the vocabulary size. It can also be useful to get the number of distinct values in a stream in constant time. It won’t give the exact value but it can be used to get an estimate of the number of distinct values. It can be calculated using the below expr. For proof, you can go through the book. No. of distinct values = \\(O(M/Minwise Hash)\\) . Here M is the size of the output space of the hashing function. ",
    "url": "http://localhost:4000/notes/17562e75-6070-454a-bae1-abfca780bc6d.html#minwise-hashing",
    "relUrl": "/notes/17562e75-6070-454a-bae1-abfca780bc6d.html#minwise-hashing"
  },"7": {
    "doc": "Chapter 6",
    "title": "Chapter 6",
    "hpath": "Algorithm.DesignManualBook.chap6",
    "content": " ",
    "url": "http://localhost:4000/notes/17562e75-6070-454a-bae1-abfca780bc6d.html",
    "relUrl": "/notes/17562e75-6070-454a-bae1-abfca780bc6d.html"
  },"8": {
    "doc": "About me",
    "title": "About Me",
    "hpath": "self",
    "content": "My name is Prabhsimrandeep Singh. Currently I work in Juniper networks as a timing engineer. I work on implemeting clocking and synchronization protocols like syncE and PTP. I am a big fan of TV shows. In my free time, I go to hacker news to get some tech gossip. ",
    "url": "http://localhost:4000/notes/1d49cbf3-7a35-4a45-a047-c0170981979c.html#about-me",
    "relUrl": "/notes/1d49cbf3-7a35-4a45-a047-c0170981979c.html#about-me"
  },"9": {
    "doc": "About me",
    "title": "About me",
    "hpath": "self",
    "content": " ",
    "url": "http://localhost:4000/notes/1d49cbf3-7a35-4a45-a047-c0170981979c.html",
    "relUrl": "/notes/1d49cbf3-7a35-4a45-a047-c0170981979c.html"
  },"10": {
    "doc": "Leetcode",
    "title": "LeetCode",
    "hpath": "Algorithm.DesignManualBook.chap5.Leetcode",
    "content": " ",
    "url": "http://localhost:4000/notes/3b09ec8e-2c2c-4db7-8fff-2d857d340d44.html#leetcode",
    "relUrl": "/notes/3b09ec8e-2c2c-4db7-8fff-2d857d340d44.html#leetcode"
  },"11": {
    "doc": "Leetcode",
    "title": "Median of Two Sorted Arrays",
    "hpath": "Algorithm.DesignManualBook.chap5.Leetcode",
    "content": "Explanation . ",
    "url": "http://localhost:4000/notes/3b09ec8e-2c2c-4db7-8fff-2d857d340d44.html#median-of-two-sorted-arrays",
    "relUrl": "/notes/3b09ec8e-2c2c-4db7-8fff-2d857d340d44.html#median-of-two-sorted-arrays"
  },"12": {
    "doc": "Leetcode",
    "title": "Count of Range Sum",
    "hpath": "Algorithm.DesignManualBook.chap5.Leetcode",
    "content": "Explanation . ",
    "url": "http://localhost:4000/notes/3b09ec8e-2c2c-4db7-8fff-2d857d340d44.html#count-of-range-sum",
    "relUrl": "/notes/3b09ec8e-2c2c-4db7-8fff-2d857d340d44.html#count-of-range-sum"
  },"13": {
    "doc": "Leetcode",
    "title": "Leetcode",
    "hpath": "Algorithm.DesignManualBook.chap5.Leetcode",
    "content": " ",
    "url": "http://localhost:4000/notes/3b09ec8e-2c2c-4db7-8fff-2d857d340d44.html",
    "relUrl": "/notes/3b09ec8e-2c2c-4db7-8fff-2d857d340d44.html"
  },"14": {
    "doc": 404,
    "title": "404",
    "hpath": null,
    "content": "Page not found . ",
    "url": "http://localhost:4000/404.html#404",
    "relUrl": "/404.html#404"
  },"15": {
    "doc": 404,
    "title": 404,
    "hpath": null,
    "content": " ",
    "url": "http://localhost:4000/404.html",
    "relUrl": "/404.html"
  },"16": {
    "doc": "Chapter 8",
    "title": "Weighted Graphs",
    "hpath": "Algorithm.DesignManualBook.chap8",
    "content": " ",
    "url": "http://localhost:4000/notes/432f6b85-3c1e-44e3-8c5c-e4634746ad2b.html#weighted-graphs",
    "relUrl": "/notes/432f6b85-3c1e-44e3-8c5c-e4634746ad2b.html#weighted-graphs"
  },"17": {
    "doc": "Chapter 8",
    "title": "Minimum Spanning Trees",
    "hpath": "Algorithm.DesignManualBook.chap8",
    "content": "We can get the MSTs using algorithms like Prim’s Algorithm and Krushkal’s Algorithm. I am going to talk about some of the extensions of this problem. Here are some of the interesting ones: . | Maximum spanning trees - The maximum spanning tree of any graph can be found by simply negating the weights of all edges and running Prim’s or Kruskal’s algorithm. The most negative spanning tree in the negated graph is the maximum spanning tree in the original. | Minimum product spanning trees – Suppose we seek the spanning tree that minimizes the product of edge weights, assuming all edge weights are positive. Since lg(a · b) = lg(a) + lg(b), the minimum spanning tree on a graph whose edge weights are replaced with their logarithms gives the minimum product spanning tree on the original graph. | Minimum bottleneck spanning tree – Sometimes we seek a spanning tree that minimizes the maximum edge weight over all possible trees. In fact, every minimum spanning tree has this property. The proof follows directly from the correctness of Kruskal’s algorithm. Such bottleneck spanning trees have interesting applications when the edge weights are interpreted as costs, capacities, or strengths. A less efficient but conceptually simpler way to solve such problems might be to delete all “heavy” edges from the graph and ask whether the result is still connected. These kinds of tests can be done with BFS or DFS. | . ",
    "url": "http://localhost:4000/notes/432f6b85-3c1e-44e3-8c5c-e4634746ad2b.html#minimum-spanning-trees",
    "relUrl": "/notes/432f6b85-3c1e-44e3-8c5c-e4634746ad2b.html#minimum-spanning-trees"
  },"18": {
    "doc": "Chapter 8",
    "title": "Shortest Path",
    "hpath": "Algorithm.DesignManualBook.chap8",
    "content": "Guru Mantra : Try to design graphs, not algorithms Context - Suppose we are given a directed graph whose weights are on the vertices instead of the edges. Thus, the cost of a path from x to y is the sum of the weights of all vertices on the path. Give an efficient algorithm for finding shortest paths on vertex-weighted graphs. Solution: There are two approaches here: . | Adapt the algorithm we have for edge-weighted graphs (Dijkstra’s) to the new vertex-weighted domain. It should be clear that this will work. We replace any reference to the weight of any directed edge (x, y) with the weight of the destination vertex y. This can be looked up as needed from an array of vertex weights. | Other way to keep the algorithm intact and instead concentrate on constructing an edge-weighted graph on which Dijkstra’s algorithm will give the desired answer. Set the weight of each directed edge (i, j) in the input graph to the cost of vertex j. Dijkstra’s algorithm now does the job. | . ",
    "url": "http://localhost:4000/notes/432f6b85-3c1e-44e3-8c5c-e4634746ad2b.html#shortest-path",
    "relUrl": "/notes/432f6b85-3c1e-44e3-8c5c-e4634746ad2b.html#shortest-path"
  },"19": {
    "doc": "Chapter 8",
    "title": "Chapter 8",
    "hpath": "Algorithm.DesignManualBook.chap8",
    "content": " ",
    "url": "http://localhost:4000/notes/432f6b85-3c1e-44e3-8c5c-e4634746ad2b.html",
    "relUrl": "/notes/432f6b85-3c1e-44e3-8c5c-e4634746ad2b.html"
  },"20": {
    "doc": "Chapter 5",
    "title": "Chapter 5",
    "hpath": "Algorithm.DesignManualBook.chap5",
    "content": " ",
    "url": "http://localhost:4000/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html",
    "relUrl": "/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html"
  },"21": {
    "doc": "Chapter 5",
    "title": "Divide And Conquer",
    "hpath": "Algorithm.DesignManualBook.chap5",
    "content": "This is one of those things which can come handy during unusual times. Binary search is a well known algorithm which has a lot of power. But sometimes it can be tricky to see all its usecases. I will discuss some of them here. ",
    "url": "http://localhost:4000/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html#divide-and-conquer",
    "relUrl": "/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html#divide-and-conquer"
  },"22": {
    "doc": "Chapter 5",
    "title": "Binary Search",
    "hpath": "Algorithm.DesignManualBook.chap5",
    "content": "int binarySearch(int arr[], int l, int r, int x) { while (l &lt;= r) { int m = l + (r - l) / 2; // Check if x is present at mid if (arr[m] == x) return m; // If x greater, ignore left half if (arr[m] &lt; x) l = m + 1; // If x is smaller, ignore right half else r = m - 1; } // if we reach here, then element was // not present return -1; } . Above algorithm is useful when we have single instance of x in array. Suppose we have multiple instances in the array. In this case, we will be more interested in getting the bounds b/w which the values appear. In that case, we don’t need to exit the loop. we can iterate till we reach one of the bounds. /* if x is present in arr[] then returns the index of FIRST occurrence of x in arr[0..n-1], otherwise returns -1 */ int first(int arr[], int x, int n) { int low = 0, high = n - 1, res = -1; while (low &lt;= high) { // Normal Binary Search Logic int mid = (low + high) / 2; if (arr[mid] &gt; x) high = mid - 1; else if (arr[mid] &lt; x) low = mid + 1; // If arr[mid] is same as x, we // update res and move to the left // half. else { res = mid; high = mid - 1; } } return res; } /* if x is present in arr[] then returns the index of LAST occurrence of x in arr[0..n-1], otherwise returns -1 */ int last(int arr[], int x, int n) { int low = 0, high = n - 1, res = -1; while (low &lt;= high) { // Normal Binary Search Logic int mid = (low + high) / 2; if (arr[mid] &gt; x) high = mid - 1; else if (arr[mid] &lt; x) low = mid + 1; // If arr[mid] is same as x, we // update res and move to the right // half. else { res = mid; low = mid + 1; } } return res; } . ",
    "url": "http://localhost:4000/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html#binary-search",
    "relUrl": "/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html#binary-search"
  },"23": {
    "doc": "Chapter 5",
    "title": "Recurrence Relations",
    "hpath": "Algorithm.DesignManualBook.chap5",
    "content": "These relations are useful for Divide and conquer. The recurrences are of the form. \\(T(n) = a · T(n/b) + f(n)\\) . | If \\(f(n) = O(n^{log_b(a - e)})\\) for some constant \u0002\\(e &gt; 0\\), then \\(T(n) = Θ(n^{log_b a})\\). | If \\(f(n) = Θ(n^{log_b a})\\), then \\(T(n) = Θ(n^{log_b a} lg n)\\). | If \\(f(n) = O(n^{log_b a + e})\\) for some constant \u0002\\(e &gt; 0\\), and if \\(a.f(n/b) [&lt;=] c.f(n)\\) for some \\(c [&lt;] 1\\), then \\(T(n) = Θ(f(n))\\). | . ",
    "url": "http://localhost:4000/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html#recurrence-relations",
    "relUrl": "/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html#recurrence-relations"
  },"24": {
    "doc": "Chapter 5",
    "title": "Fast Convolution",
    "hpath": "Algorithm.DesignManualBook.chap5",
    "content": "Divide and conquer can be used here as well. We can solve these kind of problems using FFT in O(log n) . ",
    "url": "http://localhost:4000/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html#fast-convolution",
    "relUrl": "/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html#fast-convolution"
  },"25": {
    "doc": "Chapter 5",
    "title": "Parallelism",
    "hpath": "Algorithm.DesignManualBook.chap5",
    "content": "Divide and conquer is used widely because it can enable parallel processing. But It may not be that easy. ",
    "url": "http://localhost:4000/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html#parallelism",
    "relUrl": "/notes/6322e98c-5f20-4cfc-9967-1ba4f7211957.html#parallelism"
  },"26": {
    "doc": "DesignManualBook",
    "title": "DesignManualBook",
    "hpath": "Algorithm.DesignManualBook",
    "content": " ",
    "url": "http://localhost:4000/notes/65c15828-e141-4f71-9f6c-57bd57bf3481.html",
    "relUrl": "/notes/65c15828-e141-4f71-9f6c-57bd57bf3481.html"
  },"27": {
    "doc": "Exercises",
    "title": "Exercises",
    "hpath": "Algorithm.DesignManualBook.chap4.Exercises",
    "content": ". | 4.4 - Counting sort . | 4-11 Design an O(n) algorithm that, given a list of n elements, finds all the elements that appear more than n/2 times in the list. Then, design an O(n) algorithm that, given a list of n elements, finds all the elements that appear more than \\(n/4\\) times. We can get the solution using something similar to tetris Game. Credits: GoG. we can solve for any n/k in O(nk) time. Here is the intuition: . | create a temp array of size k-1 | iterate over the elements of the array. For each element add it into temp array and increment its count | when the temp array is filled and a new element comes, delete the count of each entry in the temp array and ignore the element. | follow the above steps till the end. | now user needs to verify for each element y in temp, iterating over the array. to make sure each element occurs more than n/k times. | . | . void moreThanNdK(int arr[], int n, int k) { // k must be greater than // 1 to get some output if (k &lt; 2) return; /* Step 1: Create a temporary array (contains element and count) of size k-1. Initialize count of all elements as 0 */ struct eleCount temp[k - 1]; for (int i = 0; i &lt; k - 1; i++) temp[i].c = 0; /* Step 2: Process all elements of input array */ for (int i = 0; i &lt; n; i++) { int j; /* If arr[i] is already present in the element count array, then increment its count */ for (j = 0; j &lt; k - 1; j++) { if (temp[j].e == arr[i]) { temp[j].c += 1; break; } } /* If arr[i] is not present in temp[] */ if (j == k - 1) { int l; /* If there is position available in temp[], then place arr[i] in the first available position and set count as 1*/ for (l = 0; l &lt; k - 1; l++) { if (temp[l].c == 0) { temp[l].e = arr[i]; temp[l].c = 1; break; } } /* If all the position in the temp[] are filled, then decrease count of every element by 1 */ if (l == k - 1) for (l = 0; l &lt; k; l++) temp[l].c -= 1; } . ",
    "url": "http://localhost:4000/notes/6b9c2b74-68aa-468e-a8fb-44925c159f75.html",
    "relUrl": "/notes/6b9c2b74-68aa-468e-a8fb-44925c159f75.html"
  },"28": {
    "doc": "Home",
    "title": "Welcome to the DEN",
    "hpath": "root",
    "content": "This is the root of my vault. If you decide to visit my github page, this will be your landing page. I have build this website using dendron. It’s an open source project. Really Awesome. ",
    "url": "http://localhost:4000/#welcome-to-the-den",
    "relUrl": "/#welcome-to-the-den"
  },"29": {
    "doc": "Home",
    "title": "What’s it about",
    "hpath": "root",
    "content": "I plan to use this to jot down my notes for some of the problems I have worked on. ",
    "url": "http://localhost:4000/#whats-it-about",
    "relUrl": "/#whats-it-about"
  },"30": {
    "doc": "Home",
    "title": "Home",
    "hpath": "root",
    "content": " ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"31": {
    "doc": "Chapter 9",
    "title": "Combinatorial Search",
    "hpath": "Algorithm.DesignManualBook.chap9",
    "content": "Recursion has been taught to us as part of the early lessons in programming. We know that it can be used to solve many problems but it has a lot of computational cost. To save cycles, we have techniques which can be used for removing some unnecessary paths. I was aware of techniques like pruning. But I have learnt many others from this book. Here are some of the cool ones: . ",
    "url": "http://localhost:4000/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html#combinatorial-search",
    "relUrl": "/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html#combinatorial-search"
  },"32": {
    "doc": "Chapter 9",
    "title": "Backtracking",
    "hpath": "Algorithm.DesignManualBook.chap9",
    "content": "Run through all of the search space and Wham!! you get the answer. ",
    "url": "http://localhost:4000/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html#backtracking",
    "relUrl": "/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html#backtracking"
  },"33": {
    "doc": "Chapter 9",
    "title": "Best-First Search",
    "hpath": "Algorithm.DesignManualBook.chap9",
    "content": "This has been a useful technique that I’ve learned recently. By common sense, to speed up search it is better to explore your best options before the less promising choices. An excerpt from the book: . Best-first search, also called branch and bound, assigns a cost to every partial solution we have generated. We use a priority queue to keep track of these partial solutions by cost, so the most promising partial solution can be easily identified and expanded. As in backtracking, we explore the next partial solution by testing if it is a solution and calling process solution if it is. We identify all ways to expand this partial solution by calling construct candidates, each of which gets inserted into the priority queue with its associated cost. More Info here. ",
    "url": "http://localhost:4000/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html#best-first-search",
    "relUrl": "/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html#best-first-search"
  },"34": {
    "doc": "Chapter 9",
    "title": "The A* Heuristic",
    "hpath": "Algorithm.DesignManualBook.chap9",
    "content": "This heuristic is an extension of best-first search. Instead of only comparing the partial costs, we try to estimated the total costs using some smart guesses. Here our cost function is f(n) which is calculated as follow: \\(f(n) = g(n) + h(n)\\) Here, h(n) = the estimated movement cost from here to target and, g(n) = cost to move from the starting point to this point . An excerpt from the book: . The idea is to use a lower bound on the cost of all possible partial solution extensions that is stronger than just the cost of the current partial tour. This will make promising partial solutions look more interesting than those that have the fewest vertices. More Info here. ",
    "url": "http://localhost:4000/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html#the-a-heuristic",
    "relUrl": "/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html#the-a-heuristic"
  },"35": {
    "doc": "Chapter 9",
    "title": "Reverse Search",
    "hpath": "Algorithm.DesignManualBook.chap9",
    "content": "I have learnt about this trick while going through this problem . Suppose we have to recurse for several steps(~n). It may be the case that about n/2 steps can be done in a feasible time as there are less cases to deal with. What we can do is backtrack from the target and create a tree of about n/2 depth. Similarly we can create a tree from src of about n/2 depth. Then we can find the shortest path from src to target by comparing the entries created in both trees. it can be represented as something like this: . src / \\ / \\ / \\ some common entries which were reachable in both \\ / \\ / \\ / target . ",
    "url": "http://localhost:4000/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html#reverse-search",
    "relUrl": "/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html#reverse-search"
  },"36": {
    "doc": "Chapter 9",
    "title": "Chapter 9",
    "hpath": "Algorithm.DesignManualBook.chap9",
    "content": " ",
    "url": "http://localhost:4000/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html",
    "relUrl": "/notes/95e6ff27-ade3-4197-9557-00a6040f15d5.html"
  },"37": {
    "doc": "Coding Problems",
    "title": "LeetCode",
    "hpath": "Algorithm.DesignManualBook.chap4.LeetCode",
    "content": " ",
    "url": "http://localhost:4000/notes/e05247c2-9939-4f60-a5d5-52c4280d92e7.html#leetcode",
    "relUrl": "/notes/e05247c2-9939-4f60-a5d5-52c4280d92e7.html#leetcode"
  },"38": {
    "doc": "Coding Problems",
    "title": "Binary Index Tree/ Fenwick Tree",
    "hpath": "Algorithm.DesignManualBook.chap4.LeetCode",
    "content": "This is useful for cases when we need to perform two types of operation on an array: . | updating arr[i] | operation which involves arr[0],…, arr[i] eg: sum(0, a[i]), max(0, a[i]) | . We can see that in normal case, update step takes O(1) time and 2nd operation takes O(n) time. Using BIT, we can run both these steps at O(log n). Same is the use case for segment trees. Here we will consider BIT tree where the 2nd operation is sum. First step is the update step. Here is the code(credits:GOG): . // Returns sum of arr[0..index]. This function assumes // that the array is preprocessed and partial sums of // array elements are stored in BITree[]. int getSum(int BITree[], int index) { int sum = 0; // Initialize result // index in BITree[] is 1 more than the index in arr[] index = index + 1; // Traverse ancestors of BITree[index] while (index&gt;0) { // Add current element of BITree to sum sum += BITree[index]; // Move index to parent node in getSum View // Intuition: // index = a1b (where b are all zeros and 1 represents the least set bit) // we have -index = 2's complement of index i.e. a'0b' + 1 = a'1b //index &amp; (-index) = a1b &amp; a'1b = 010 (least set bit of index) // in this step we are removing the last set bit of index in each iteration // index &amp; (-index) index -= index &amp; (-index); } return sum; } // Updates a node in Binary Index Tree (BITree) at given index // in BITree. The given value 'val' is added to BITree[i] and // all of its ancestors in tree. void updateBIT(int BITree[], int n, int index, int val) { // index in BITree[] is 1 more than the index in arr[] index = index + 1; // Traverse all ancestors and add 'val' while (index &lt;= n) { // Add 'val' to current node of BI Tree BITree[index] += val; // Update index to that of parent in update View index += index &amp; (-index); } } . ",
    "url": "http://localhost:4000/notes/e05247c2-9939-4f60-a5d5-52c4280d92e7.html#binary-index-tree-fenwick-tree",
    "relUrl": "/notes/e05247c2-9939-4f60-a5d5-52c4280d92e7.html#binary-index-tree-fenwick-tree"
  },"39": {
    "doc": "Coding Problems",
    "title": "Coding Problems",
    "hpath": "Algorithm.DesignManualBook.chap4.LeetCode",
    "content": " ",
    "url": "http://localhost:4000/notes/e05247c2-9939-4f60-a5d5-52c4280d92e7.html",
    "relUrl": "/notes/e05247c2-9939-4f60-a5d5-52c4280d92e7.html"
  },"40": {
    "doc": "Algorithms",
    "title": "Algorithms",
    "hpath": "Algorithm",
    "content": "These are my Notes when I was reading the book: The Algorithm Design manual . I have jotted down the solutions for some of the coding problems I liked. ",
    "url": "http://localhost:4000/notes/e86ac3ab-dbe1-47a1-bcd7-9df0d0490b40.html",
    "relUrl": "/notes/e86ac3ab-dbe1-47a1-bcd7-9df0d0490b40.html"
  },"41": {
    "doc": "Chapter 4",
    "title": "Sorting",
    "hpath": "Algorithm.DesignManualBook.chap4",
    "content": " ",
    "url": "http://localhost:4000/notes/fe757956-ec54-49f4-a59c-87a93281efef.html#sorting",
    "relUrl": "/notes/fe757956-ec54-49f4-a59c-87a93281efef.html#sorting"
  },"42": {
    "doc": "Chapter 4",
    "title": "Heap Sort",
    "hpath": "Algorithm.DesignManualBook.chap4",
    "content": "Constructing a heap takes O(n) time. removing min/max element causes O(log n) which turn to O(nlog n) for n elements. Selection Sort - we partition the input array into sorted and unsorted regions. To find the smallest item, we performed a linear sweep through the unsorted portion of the array. The smallest item is then swapped with the \\(ith\\) item in the array before moving on to the next iteration. Selection sort performs n iterations, where the average iteration takes \\(n/2\\) steps, for a total of \\(O(n^2)\\) time. Heap Sort can be seen as improved version of selection sort. We are just using a better data structure. In selection sort, swap takes \\(O(1)\\) time. but getting minimum item takes \\(O(n)\\). But we can use a data structure like priority queues/ BST which has \\(O(n)\\) search time which reduces time complexity to \\(O(nlog n)\\) . ",
    "url": "http://localhost:4000/notes/fe757956-ec54-49f4-a59c-87a93281efef.html#heap-sort",
    "relUrl": "/notes/fe757956-ec54-49f4-a59c-87a93281efef.html#heap-sort"
  },"43": {
    "doc": "Chapter 4",
    "title": "Merge Sort",
    "hpath": "Algorithm.DesignManualBook.chap4",
    "content": "The most important thing to remember here is that divide and conquer is your friend. It’s always useful. so keep it in mind when going through the problems. you can use it a lot of times. Not just for Sorting. ",
    "url": "http://localhost:4000/notes/fe757956-ec54-49f4-a59c-87a93281efef.html#merge-sort",
    "relUrl": "/notes/fe757956-ec54-49f4-a59c-87a93281efef.html#merge-sort"
  },"44": {
    "doc": "Chapter 4",
    "title": "Quick Sort",
    "hpath": "Algorithm.DesignManualBook.chap4",
    "content": "Have read this too many times. But now comes the intuition. One thing which is important is elements less than p are in range [0, firsthigh) so firsthigh is never part of that bunch. So when we swap s[i] and s[firsthigh], we are just swaping s[i](&lt; p) with s[firsthigh](&gt;= p). also we don’t include pivot element in the loop because it won’t make a change as for i=p, s[i] = s[p]. p = h; /* select last element as pivot */ firsthigh = l; for (i = l; i &lt; h; i++) { if (s[i] &lt; s[p]) { swap(&amp;s[i], &amp;s[firsthigh]); firsthigh++; } } swap(&amp;s[p], &amp;s[firsthigh]); . Randomized Algorithms . Quicksort can be improved upon by randomizing the input set. This can still give the worst case but it won’t be dependent on the input itself. Such randomization can help us in preventing worst case for a certain input. ",
    "url": "http://localhost:4000/notes/fe757956-ec54-49f4-a59c-87a93281efef.html#quick-sort",
    "relUrl": "/notes/fe757956-ec54-49f4-a59c-87a93281efef.html#quick-sort"
  },"45": {
    "doc": "Chapter 4",
    "title": "Wiggle Sort",
    "hpath": "Algorithm.DesignManualBook.chap4",
    "content": "This can be done in O(n) time by doing a single traversal of given array. The idea is based on the fact that if we make sure that all even positioned (at index 0, 2, 4, ..) elements are greater than their adjacent odd elements, we don’t need to worry about odd positioned element. // This function sorts arr[0..n-1] in wave form, i.e., arr[0] &gt;= // arr[1] &lt;= arr[2] &gt;= arr[3] &lt;= arr[4] &gt;= arr[5] .... void sortInWave(int arr[], int n) { // Traverse all even elements for (int i = 0; i &lt; n; i+=2) { // If current even element is smaller than previous if (i&gt;0 &amp;&amp; arr[i-1] &gt; arr[i] ) swap(&amp;arr[i], &amp;arr[i-1]); // If current even element is smaller than next if (i&lt;n-1 &amp;&amp; arr[i] &lt; arr[i+1] ) swap(&amp;arr[i], &amp;arr[i + 1]); } } . ",
    "url": "http://localhost:4000/notes/fe757956-ec54-49f4-a59c-87a93281efef.html#wiggle-sort",
    "relUrl": "/notes/fe757956-ec54-49f4-a59c-87a93281efef.html#wiggle-sort"
  },"46": {
    "doc": "Chapter 4",
    "title": "Chapter 4",
    "hpath": "Algorithm.DesignManualBook.chap4",
    "content": " ",
    "url": "http://localhost:4000/notes/fe757956-ec54-49f4-a59c-87a93281efef.html",
    "relUrl": "/notes/fe757956-ec54-49f4-a59c-87a93281efef.html"
  }
}
